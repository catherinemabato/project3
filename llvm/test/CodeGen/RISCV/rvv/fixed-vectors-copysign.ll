; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=riscv32 -target-abi=ilp32d -mattr=+v,+zvfh -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,ZVFH
; RUN: llc -mtriple=riscv64 -target-abi=lp64d -mattr=+v,+zvfh -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,ZVFH
; RUN: llc -mtriple=riscv32 -target-abi=ilp32d -mattr=+v,+zvfhmin -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,ZVFHMIN
; RUN: llc -mtriple=riscv64 -target-abi=lp64d -mattr=+v,+zvfhmin -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,ZVFHMIN

; Single-width copysign tests are in fixed-vectors-fp.ll

define void @copysign_neg_trunc_v4f16_v4f32(ptr %x, ptr %y) {
; ZVFH-LABEL: copysign_neg_trunc_v4f16_v4f32:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; ZVFH-NEXT:    vle32.v v8, (a1)
; ZVFH-NEXT:    vle16.v v9, (a0)
; ZVFH-NEXT:    vfncvt.f.f.w v10, v8
; ZVFH-NEXT:    vfsgnjn.vv v8, v9, v10
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: copysign_neg_trunc_v4f16_v4f32:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vle32.v v9, (a1)
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    addi a2, a1, -1
; ZVFHMIN-NEXT:    vand.vx v8, v8, a2
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v9
; ZVFHMIN-NEXT:    vxor.vx v9, v10, a1
; ZVFHMIN-NEXT:    vand.vx v9, v9, a1
; ZVFHMIN-NEXT:    vor.vv v8, v8, v9
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <4 x half>, ptr %x
  %b = load <4 x float>, ptr %y
  %c = fneg <4 x float> %b
  %d = fptrunc <4 x float> %c to <4 x half>
  %e = call <4 x half> @llvm.copysign.v4f16(<4 x half> %a, <4 x half> %d)
  store <4 x half> %e, ptr %x
  ret void
}

define void @copysign_neg_trunc_v3f16_v3f32(ptr %x, ptr %y) {
; ZVFH-LABEL: copysign_neg_trunc_v3f16_v3f32:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 3, e32, m1, ta, ma
; ZVFH-NEXT:    vle32.v v8, (a1)
; ZVFH-NEXT:    vle16.v v9, (a0)
; ZVFH-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; ZVFH-NEXT:    vfncvt.f.f.w v10, v8
; ZVFH-NEXT:    vsetivli zero, 3, e16, mf2, ta, ma
; ZVFH-NEXT:    vfsgnjn.vv v8, v9, v10
; ZVFH-NEXT:    vse16.v v8, (a0)
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: copysign_neg_trunc_v3f16_v3f32:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 3, e16, mf2, ta, ma
; ZVFHMIN-NEXT:    vle16.v v8, (a0)
; ZVFHMIN-NEXT:    vle32.v v9, (a1)
; ZVFHMIN-NEXT:    lui a1, 8
; ZVFHMIN-NEXT:    addi a2, a1, -1
; ZVFHMIN-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; ZVFHMIN-NEXT:    vand.vx v8, v8, a2
; ZVFHMIN-NEXT:    vfncvt.f.f.w v10, v9
; ZVFHMIN-NEXT:    vxor.vx v9, v10, a1
; ZVFHMIN-NEXT:    vand.vx v9, v9, a1
; ZVFHMIN-NEXT:    vsetivli zero, 3, e16, mf2, ta, ma
; ZVFHMIN-NEXT:    vor.vv v8, v8, v9
; ZVFHMIN-NEXT:    vse16.v v8, (a0)
; ZVFHMIN-NEXT:    ret
  %a = load <3 x half>, ptr %x
  %b = load <3 x float>, ptr %y
  %c = fneg <3 x float> %b
  %d = fptrunc <3 x float> %c to <3 x half>
  %e = call <3 x half> @llvm.copysign.v3f16(<3 x half> %a, <3 x half> %d)
  store <3 x half> %e, ptr %x
  ret void
}

define void @copysign_neg_ext_v2f64_v2f32(ptr %x, ptr %y) {
; CHECK-LABEL: copysign_neg_ext_v2f64_v2f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; CHECK-NEXT:    vle32.v v8, (a1)
; CHECK-NEXT:    vle64.v v9, (a0)
; CHECK-NEXT:    vfwcvt.f.f.v v10, v8
; CHECK-NEXT:    vsetvli zero, zero, e64, m1, ta, ma
; CHECK-NEXT:    vfsgnjn.vv v8, v9, v10
; CHECK-NEXT:    vse64.v v8, (a0)
; CHECK-NEXT:    ret
  %a = load <2 x double>, ptr %x
  %b = load <2 x float>, ptr %y
  %c = fneg <2 x float> %b
  %d = fpext <2 x float> %c to <2 x double>
  %e = call <2 x double> @llvm.copysign.v2f64(<2 x double> %a, <2 x double> %d)
  store <2 x double> %e, ptr %x
  ret void
}
